---
title: 'Activity tracking: Predicting exercise execution'
author: "Rona1d"
date: "august 6, 2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r Loading_packages, warning=FALSE, message=FALSE}
library(caret)
```
### Introduction

*This report was written in fulfillment of the assignment of week 4 of the 'Practical Machine Learning' course within the Coursera specialization track 'Data Science'.*

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this report data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants performing exercises is used. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways (more information is available [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset)). Goal of this assignment is, by looking at the data, to predict in which way the exercises were performed. 

### Executive summary

Having analysed the data with machine learning techniques, we were able to accurately predict which type of exercise was performed. In particular the *Random Forest* model performed very well with an accuracy of 99.6%

### Getting and cleaning data
For this research we will be using data on exercises provided through the Coursera course website. The data is devided in a training and test set.

```{r}

# First, make sure a folder named 'data' exists
if(!file.exists("data")) {
        dir.create("data")
}
setwd("./data")

# Downloading file...
fileURL1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileURL2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!file.exists("HAT_traindata.csv")) {
        download.file(fileURL1, "./HAT_traindata.csv")
}
if(!file.exists("HAT_testdata.csv")) {
        download.file(fileURL2, "./HAT_testdata.csv")
}

# A quick preliminary scan of the data reveiled variables with a lot of blanks.
# It is therefore wise to name the blanks as 'NA' in the read.csv step
trainset <- read.csv("HAT_traindata.csv", na.strings=c(""," ","NA"))
valset <- read.csv("HAT_testdata.csv")
```

The data in the trainset (`r nrow(trainset)` rows and `r ncol(trainset)` columns) shows a lot of colums with a lot of missing values. To be specific, columns with more than 95% of NA's will be deleted from the dataset as they are of little use (please consult the Appendix for more detailed information about the variables). Note that the first 7 columns in the dataset are not providing much useful data either so those too wil be deleted.

```{r cleaning_data}
trainset2 <- trainset[,colSums(is.na(trainset))<(nrow(trainset)*0.95)]
trainset3 <- subset(trainset2, select=-c(1:7)); rm(trainset2)
```

The final input dataset now has 'only' `r ncol(trainset3)` columns left. To be able to perform cross validation on any model to be built, the dataset has to partitioned into a training set (80%) and a testing set (20%).

```{r partitioning_dataset}
set.seed(1234)
inTrain = createDataPartition(trainset3$classe, p = 0.8)[[1]]
training = trainset3[ inTrain,]
testing = trainset3[-inTrain,]
```

### Choosing and running models
The first choice for a model would be a *Random Forest* algorithm ("rf"). This type of model is very good for classification problems like the one in this report. It also, in general, outperforms (generalised) linear models when the relation between variables in a dataset is non-linear. For sake of comparison, a *Stochastic Gradient Boosting* ("gbm") model, which is another type of model based on decision trees, will also be fit on the dataset.

```{r}
#fit1 <- train(classe~., method = "rf", data=training); save(fit1, file = "fit1.RData")
#fit2 <- train(classe~., method = "gbm", data=training); save(fit2, file = "fit2.RData")
#Note that the above fit will need to run when not run before. 
#Once these fits are saved, one can pull the models from memory with the 'load' statement
load(file = "fit1.RData")
load(file = "fit2.RData")
```

### Model performance

By predicting the fit models on the test dataset, the model performance can be established. Accuracy is measured by comparing the predicted values to the actual values in the test dataset.

```{r Cross_validation, warning=FALSE, message=FALSE}
p1 <- predict(fit1, training)
p2 <- predict(fit2, training)
p3 <- predict(fit1, testing)
p4 <- predict(fit2, testing)

c1 <- confusionMatrix(p1, training$classe)
c2 <- confusionMatrix(p2, training$classe)
c3 <- confusionMatrix(p3, testing$classe)
c4 <- confusionMatrix(p4, testing$classe)
```

On the training set, the rf model performs outstanding with an accuracy of 100%. The gbm model also has a high accuracy (97.4%) on the training set, but underperforms to the rf. The model to go with would therefore be the rf model.
Performing cross-validation on the testset, the rf model still perfoms very good with aan accuracy of 99.6%  which translates to an out of sample error rate of 0.4% (please refer to the Appendix for the model results). 
Combining both of the predictors would in general be a way to further increase accuracy, but with the current model accuracy of 99,6% that is not deemed necessary. The fitted rf model would therefore be the model of choice for predicting the performed exercises.

\newpage

### Appendix

```{r Explore variables in dataset}
str(trainset) # Below an overview of variables in the initial dataset
```

\newpage

```{r MOdel performance}
c1$overall # Accuracy of the Random Forest model on the training set
c2$overall # Accuracy of the Stochastic Gradient Boosting model on the training set
c3 # Confusion Matrix and Statistics for the Random Forest model on the test set
```
